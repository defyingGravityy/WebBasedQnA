{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391f494b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7b9422e9a5d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7b9422e9afd0>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm= ChatGroq(groq_api_key=groq_api_key, model=\"Llama3-8b-8192\")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567da5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786157d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_huggingface\n",
      "  Using cached langchain_huggingface-0.3.0-py3-none-any.whl.metadata (996 bytes)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langchain_huggingface) (0.3.68)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langchain_huggingface) (0.21.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.2 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langchain_huggingface) (0.33.2)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.4.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.10.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.1)\n",
      "Requirement already satisfied: filelock in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2025.3.2)\n",
      "Requirement already satisfied: requests in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (1.1.5)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.23.0)\n",
      "Requirement already satisfied: anyio in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (4.7.0)\n",
      "Requirement already satisfied: certifi in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/vaishnavi/anaconda3/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.3.0)\n",
      "Using cached langchain_huggingface-0.3.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: langchain_huggingface\n",
      "Successfully installed langchain_huggingface-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92951171",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3360696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "loader=WebBaseLoader(\n",
    "    web_path=(\"https://en.wikipedia.org/wiki/Artificial_intelligence\",)\n",
    "    # bs_kwargs=dict(\n",
    "    #     parse_only=bs4.SoupStrainer(\n",
    "    #         class_=(\"post-content\",\"post-title\",\"post-header\")\n",
    "    #     )\n",
    "    # ),\n",
    ")\n",
    "doc=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b39fce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7b933c135f90>, search_kwargs={})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000,chunk_overlap=200)\n",
    "splits=text_splitter.split_documents(doc)\n",
    "db = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa9e2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=(\n",
    "    \"You are an assistant for answering questions\"\n",
    "    \"Use the following pieces of context to answer the question\"\n",
    "    \"{context}\"\n",
    "    \"If you don't know the answer, just say that you don't know.\"\n",
    "    \"answer concise\"\n",
    ")\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\",\"{input}\"),\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6a0f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain=create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain=create_retrieval_chain(retriever,question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e154b85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=rag_chain.invoke({\"input\":\"What is Artificial Intelligence?\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f028f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "##adding chat history\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt=(\n",
    "    \"Given a chat history and a question\"\n",
    "    \"which might reference context in the chat history\"\n",
    "    \"Formulate a standalone question\"\n",
    "    \"that can be understood\"\n",
    "    \"without the chat history do not answer the question\"\n",
    "    \"just formulate it if needed and otherwise return it as it.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\",\"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f841ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7b933c135f90>, search_kwargs={}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7b94338f42c0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Given a chat history and a questionwhich might reference context in the chat historyFormulate a standalone questionthat can be understoodwithout the chat history do not answer the questionjust formulate it if needed and otherwise return it as it.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7b9422e9a5d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7b9422e9afd0>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7b933c135f90>, search_kwargs={})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_aware_retriever=create_history_aware_retriever(llm,retriever,contextualize_q_prompt)\n",
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb7c1b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\",\"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbeb25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain=create_stuff_documents_chain(llm,qa_prompt)\n",
    "rag_chain=create_retrieval_chain(history_aware_retriever,question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35c781e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the text, AI researchers have adapted and integrated a wide range of techniques to achieve these goals, including:\\n\\n1. Search and mathematical optimization\\n2. Formal logic\\n3. Artificial neural networks\\n4. Methods based on statistics, operations research, and economics\\n5. Drawing upon psychology, linguistics, philosophy, neuroscience, and other fields'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "chat_history=[]\n",
    "question=\"What are traditional goals of AI?\"\n",
    "response1=rag_chain.invoke({\"input\":question,\"chat_history\":chat_history})\n",
    "\n",
    "chat_history.extend(\n",
    "    [HumanMessage(content=question), AIMessage(content=response1['answer'])]\n",
    ")\n",
    "# response1['answer']\n",
    "\n",
    "question1=\"WHat they did to achieve these goals?\"\n",
    "response2=rag_chain.invoke({\"input\":question1,\"chat_history\":chat_history}) \n",
    "response2['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
